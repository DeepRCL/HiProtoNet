################### Experiment information ######################
description: Hierarchical XProtoNet in hyperbolic space
run_name: "XProtoNet_HAPPI"
agent: "Hyperbolic_XProtoNet"
wandb_mode: 'online'  # one of "online", "offline" or "disabled". disabled turns wandb logging off! good for testing
abstain_class: False

################## Model information ##########################
model: &model
  checkpoint_path: ''
  name: "Hyper_XProtoNet"
  base_architecture: 'resnet18'  # backbone
  pretrained: True
  prototype_shape: (30, 256, 1, 1)  # Modify first element to select total # of prototypes (dividable by num_classes)
  num_local_prototypes_per_class: 1  # TODO expand to have more than 1
  num_classes: 3
  prototype_activation_function: 'linear'
  add_on_layers_type: 'regular'
  feat_range_type: "Sigmoid"   # can be "Tanh" or "Sigmoid" or "CLIP-style". Set the "initialize_alpha" accordingly
  initialize_alpha: False   # (TODO EXPLORE) to initialize the visual_alpha based on the first batch for stability in hyperbolic calculations
  cls_method: 'both' # broad, local, or both
  local_prototype_method: 'attn' # attn or last_layer  (last_layer only accepts local_feat_method=1_per_input)
  local_feat_method: '1_per_input'  # can be 1_per_input or 1_per_class
  lift_prototypes: False  # to lift prototypes up onto the hyperboloid or assume they are already there!
  learn_curv: False  # to learn the curvature of the hyperboloid
  local_prot_donut: 1.0

################## Training information ##########################
train: &train
  seed: 200
  num_train_epochs: 50
  save: True
  save_step: null
  num_warm_epochs: 5
  batch_size: 20
  accumulation_steps: 5
  push_start: 10
  push_rate: 5 # epochs  TODO try 5
  push_local_prots: True
  num_workers: 8
  ce_local_weight: 0.1  # if cls_method is both, this is used for local ,and its 1-value is used for broad!  so far all was 0.5
  local_loss: True # true or false for local cluster and separation losses

  criterion:
    CeLoss:
      loss_weight: 1
      reduction: 'mean'
####################################  broad ones
    ClusterPatch:  # TODO CHECK THIS
      loss_weight: 0.8
      reduction: 'mean'
    SeparationPatch:  # TODO CHECK THIS
      loss_weight: 0.08
      strategy: "all"  # can be "all" (for all other classes) or "closest" (for only the closest class)
      normalize: True  # only works when strategy is "all"
      reduction: 'mean'
####################################   local ones
    ClusterPatch_g:  # TODO CHECK THIS
      loss_weight: 0.1
      reduction: 'mean'
    SeparationPatch_g:  # TODO CHECK THIS
      loss_weight: 0.01
      strategy: "all"  # can be "all" (for all other classes) or "closest" (for only the closest class)
      normalize: True  # only works when strategy is "all"
      reduction: 'mean'
####################################
    Entailment:
      loss_weight: 0.1  # TODO EXPLORE the values
      reduction: 'mean'
    HyperbolicOrthogonalityLoss:
      loss_weight:
        part: 0.1
        local: 0
        part_local: 0.25
      mode: 'all'  # to encourage diversity in each class ('per_class'), or overal ('all')
    Lnorm_occurrence:
      p: 2
      loss_weight: 0.0 # 1e-4
      reduction: 'mean'
    trans_occurrence:  # for both broad and local depending on local_prototype_method
      loss_weight: 0.001 # 1e-3
      reduction: 'mean'
    Lnorm_FC:
      p: 1
      loss_weight: 0.0001 # 1e-4

  optimizer: &optimizer
    name: 'Adam'
    joint_lrs:
      cnn_backbone: 0.0001 # 1e-4,
      add_on_layers: 0.003 # 3e-3   # TODO try 1e-3 too
      occurrence_module: 0.003 # 3e-3   # TODO try 1e-3 too
      prototype_vectors: 0.003 # 3e-3   # TODO try 1e-3 too
      local_attn_module: 0.003 # 3e-3
      local_prototype_vectors: 0.003 # 3e-3
#      logit_scale: 0.0005  #5e-4  # from MERU's work.
      curv: 0.0005  #5e-4  # from MERU's work.
      visual_alpha: 0.0005  #5e-4  # from MERU's work.
    warm_lrs:
      add_on_layers: 0.003 # 3e-3   # TODO try 1e-3 too
      occurrence_module: 0.003 # 3e-3   # TODO try 1e-3 too
      prototype_vectors: 0.003 # 3e-3   # TODO try 1e-3 too
      local_attn_module: 0.003 # 3e-3   # TODO try 1e-3 too
      local_prototype_vectors: 0.003 # 3e-3   # TODO try 1e-3 too
      curv: 0.0005  #5e-4  # from MERU's work.
      visual_alpha: 0.0005  #5e-4  # from MERU's work.
    last_layer_lr: 0.0001 # 1e-4

  lr_schedule: &lr_schedule   # for joint_optimizer and last_layer_optimizer only
    # TODO names
    name: 'CosineAnnealingLR' # one of ReduceLROnPlateau, StepLR, CosineAnnealingLR
    StepLR:
      step_size: 5
      gamma: 0.9   # ProtoPNet used 0.1
    ReduceLROnPlateau:
      mode: 'max'  # used for F1 score
      factor: 0.5  # Factor by which the learning rate will be reduced
      patience: 5 # Number of epochs with no improvement after which learning rate will be reduced
      threshold: 0.0001  # Threshold for measuring the new optimum, to only focus on significant changes
      cooldown: 2
      min_lr: 0.000001  # 1e-6
      verbose: True
    CosineAnnealingLR:
      eta_min: 0.000001  # 1e-6
#################### Data information #######################
data: &data
  name: as_tom
  data_info_file: 'data/private/as_tom/annotations-all_NoLeak.csv'
  dataset_root: 'data/private/as_tom'
  sample_size: null
  sampler: 'AS'  # one of 'AS', 'random', 'bicuspid', 'AVA'
  label_scheme_name: 'tufts'  # one of 'tufts', 'binary', 'all', 'not_severe', 'as_only', 'mild_moderate', 'moderate_severe'
  view: 'all'   # one of  psax, plax, all
  modes:  ["train", "val", "test", "train_push"]

  augmentation: True
  transform_rotate_degrees: 15
  transform_min_crop_ratio: 0.7
  transform_time_dilation: 0.2
  normalize: True
  img_size: 224
  frames: 1  # 1 for image-based, 2 or more for video-based
  iterate_intervals: True # true if we get multiple images/videos in sequence during inference
  interval_unit: 'image' # image/second/cycle
  interval_quant: 1.0