################### Experiment information ######################
description: end-to-end Hierarchical Video-based XProtoNet in hyperbolic space
run_name: "XProtoNet_Video_HAPPI"
agent: "Hyperbolic_Video_XProtoNet_e2e"
wandb_mode: 'online'  # one of "online", "offline" or "disabled". disabled turns wandb logging off! good for testing
abstain_class: False

################## Model information ##########################
model: &model
  checkpoint_path: ""
  name: "Hyper_Video_XProtoNet"
  base_architecture: 'resnet2p1d_18'  # backbone
  backbone_last_layer_num: -3
  pretrained: True
  prototype_shape: (30, 256, 1, 1, 1)  # Modify first element to select total # of prototypes (dividable by num_classes)
  num_classes: 3
  num_local_prototypes_per_class: 2  # TODO expand to have more than 1
  prototype_activation_function: 'linear'
  feat_range_type: "Sigmoid"   # can be "Tanh" or "Sigmoid" or "CLIP-style". Set the "initialize_alpha" accordingly
  initialize_alpha: True   # (TODO EXPLORE) to initialize the visual_alpha based on the first batch for stability in hyperbolic calculations
  cls_method: 'both' # broad, local, or both
  local_prototype_method: 'attn' # attn or last_layer  (last_layer only accepts local_feat_method=1_per_input)
  local_feat_method: '1_per_class'  # can be 1_per_input or 1_per_class
  lift_prototypes: True  # to lift prototypes up onto the hyperboloid or assume they are already there!
  learn_curv: True  # to learn the curvature of the hyperboloid
  push_at_end: False
  local_prot_donut: 1.0
################## Training information ##########################
train: &train
  seed: 200
  num_train_epochs: 50
  save: True
  save_step: null
  num_warm_epochs: 5
  batch_size: 5
  accumulation_steps: 20
  push_start: 10
  push_rate: 5 # epochs
  num_workers: 8
  push_local_prots: True
  ce_local_weight: 0.1  # if cls_method is both, this is used for local ,and its 1-value is used for broad!  so far all was 0.5
  local_loss: True # true or false for local cluster and separation losses

  criterion:
    CeLoss:  # will be used if (abstain_class == False)
      loss_weight: 1
      reduction: 'mean'
    CeLossAbstain:  # will be used if (abstain_class == True)
        loss_weight: 1
        ab_weight: 0.1
        ab_logitpath: 'joined'
        reduction: 'mean'
####################################  broad ones
    ClusterPatch:  # TODO CHECK THIS
      loss_weight: 0.1
      reduction: 'mean'
    SeparationPatch:  # TODO CHECK THIS
      loss_weight: 0.01
      strategy: "all"  # can be "all" (for all other classes) or "closest" (for only the closest class)
      normalize: True  # only works when strategy is "all"
      reduction: 'mean'
####################################   local ones
    ClusterPatch_g:  # TODO CHECK THIS
      loss_weight: 0.1
      reduction: 'mean'
    SeparationPatch_g:  # TODO CHECK THIS
      loss_weight: 0.0
      strategy: "all"  # can be "all" (for all other classes) or "closest" (for only the closest class)
      normalize: True  # only works when strategy is "all"
      reduction: 'mean'
####################################
    Entailment:
      loss_weight: 0.1  # TODO EXPLORE the values
      reduction: 'mean'
    HyperbolicOrthogonalityLoss:
      loss_weight:
        part: 0.5
        local: 0.0
        part_local: 0.5
      mode: 'all'  # to encourage diversity in each class ('per_class'), or overal ('all')
    # TODO occurence loss for local too?
    Lnorm_occurrence:
      p: 2
      loss_weight: 0.0001 # 1e-4
      reduction: 'mean'
    trans_occurrence:
      loss_weight: 0.0001 # 1e-4
      reduction: 'mean'
    Lnorm_FC:
      p: 1
      loss_weight: 0.0001 # 1e-4

  optimizer: &optimizer
    name: 'Adam'
    mode: 'lr_disjoint'  # can be lr_same/lr_disjoint
    lr_same: 0.0001  # 1e-4
    lr_disjoint:
      cnn_backbone: 0.0001 # 1e-4,
      add_on_layers: 0.003 # 3e-3,
      occurrence_module: 0.003 # 3e-3   # TODO try 1e-3 too
      local_attn_module: 0.003 # 3e-3,
      prototype_vectors: 0.003 # 3e-3
      local_prototype_vectors: 0.003 # 3e-3
      curv: 0.0005  #5e-4  # from MERU's work.
      visual_alpha: 0.0005  #5e-4  # from MERU's work.
      last_layer: 0.001 # 1e-3
      last_layer_local: 0.001 # 1e-3
#      logit_scale: 0.0005  #5e-4  # from MERU's work.
    lr_last_layer_only: 0.000001 # 1e-6

  lr_schedule: &lr_schedule   # for joint_optimizer and last_layer_optimizer only
    # TODO names
    name: 'ReduceLROnPlateau' # one of ReduceLROnPlateau, StepLR, CosineAnnealingLR
    StepLR:
      step_size: 5
      gamma: 0.9   # ProtoPNet used 0.1
    ReduceLROnPlateau:
      mode: 'max'  # used for F1 score
      factor: 0.5  # Factor by which the learning rate will be reduced
      patience: 5 # Number of epochs with no improvement after which learning rate will be reduced
      threshold: 0.0001  # Threshold for measuring the new optimum, to only focus on significant changes
      cooldown: 2
      min_lr: 0.000001  # 1e-6
      verbose: True
    CosineAnnealingLR:
      eta_min: 0.000001  # 1e-6

#################### Data information #######################
data: &data   #TODO modify this according to your dataset class and dataloder needs
  name: "<dataset name>"
  data_info_file: 'data/<dataset name>/<dataset-csv-info>.csv'
  sample_size: null
  sampler: 'AS'  # one of 'AS', 'random', 'bicuspid', 'AVA'
  view: 'all'   # one of  psax, plax, all

  augmentation: True
  transform_rotate_degrees: 15
  transform_min_crop_ratio: 0.7
  transform_time_dilation: 0.2
  normalize: True
  img_size: 112
  frames: 32  # 1 for image-based, 2 or more for video-based
  iterate_intervals: True # true if we get multiple images/videos in sequence during inference
  interval_unit: 'cycle' # image/second/cycle
  interval_quant: 1.0